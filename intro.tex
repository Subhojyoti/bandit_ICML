In this paper, we consider the stochastic multi-armed bandit problem, a classical problem in sequential decision making. In this setting,  a learning algorithm is provided with a set of decisions (or arms) with reward distributions unknown to the algorithm. The learning proceeds in an iterative fashion, where in each round, the algorithm chooses an arm and receives a stochastic reward that is drawn from a stationary distribution specific to the arm selected.  
Given the goal of maximizing the cumulative reward, the learning algorithm faces the exploration-exploitation dilemma, i.e., in each round should the algorithm select the arm which has the highest observed mean reward so far 
(\textit{exploitation}), or should the algorithm choose a new arm to gain more knowledge of the true mean reward of the arms and thereby avert a sub-optimal greedy decision (\textit{exploration}). 

Let $r_i$, $i=1,\ldots,K$ denote the mean reward of the $i$th arm out of the $K$ arms and $r^* = \max_i r_i$ the optimal mean reward. The objective in the stochastic bandit problem is to minimize the cumulative regret, which is defined as follows:
\begin{align*}
R_{T}=r^{*}T - \sum_{i\in A} r_{i}N_{i}(T),
\end{align*}
where $T$ is the number of rounds, $N_{i}(T)=\sum_{m=1}^T I(I_m=i)$ is the number of times the algorithm has chosen arm $i$ up to round $T$.
The expected regret of an algorithm after $T$ rounds can be written as
%\newline
%\newline
\begin{align*}
\E[R_{T}]= \sum_{i=1}^K \E[N_i(T)] \Delta_i,
\end{align*}
where $\Delta_{i}=r^{*}-r_{i}$ denotes the gap between the means of the optimal arm and the $i$-th arm. 


%The problem, gets more difficult when the $\Delta_{i}$'s are smaller and arm set is larger. Also let $\Delta=min_{i\in A}\Delta_{i}$, that is it is the minimum possible gap over all arms in $A$.
                                                                                                                                          

%NEW RELATED WORK AND CONTRIBUTION

An early work involving a bandit setup is \cite{thompson1933likelihood}, where the author deals the problem of choosing between two treatments to administer on patients who come in sequentially. Following the seminal work of \cite{robbins1952some}, bandit algorithms have been extensively studied in a variety of applications. 
From a theoretical standpoint, an asymptotic lower bound for the regret was established in \cite{lai1985asymptotically}. In particular, it was shown that for any consistent allocation strategy, we have
$\liminf_{T \to \infty}\frac{\E[R_{T}]}{\log T}\geq\sum_{\{i:r_{i}<r^{*}\}}\frac{(r^{*}-r_{i})}{D(p_{i}||p^{*})},$
where $D(p_{i}||p^{*})$ is the Kullback-Leibler divergence between the reward densities $p_{i}$ and $p^{*}$, corresponding to arms with mean $r_{i}$ and $r^{*}$, respectively.

	There have been several algorithms with strong regret guarantees. For further reference we point the reader to \cite{bubeck2012bandits}. The foremost among them is UCB1 \cite{auer2002finite}, which has a regret upper bound of $O\big(\frac{K\log T}{\Delta}\big)$, where $\Delta = \min_{i:\Delta_i>0} \Delta_i$. This result is asymptotically order-optimal for the class of distributions considered. However, the worst case gap independent regret bound of UCB1  can be as bad as $O \big(\sqrt{TK\log T}\big)$.  In \cite{audibert2009minimax}, the authors propose the MOSS algorithm and establish that the worst case regret of MOSS is $O\big(\sqrt{TK}\big)$ which improves upon UCB1 by a factor of order $\sqrt{\log T}$. However, the gap-dependent regret of MOSS is  $O\big(\frac{K^{2}\log\left(T\Delta^{2}/K\right)}{\Delta}\big)$ and in certain regimes, this can be worse than even UCB1 (see \cite{audibert2009minimax},\cite{lattimore2015optimally}). The UCB-Improved algorithm, proposed in \cite{auer2010ucb}, is a round-based algorithm\footnote{An algorithm is \textit{round-based} if it pulls all the arms equal number of times in each round and then proceeds to eliminate one or more arms that it identifies to be sub-optimal.} variant of UCB1 that 
has a gap-dependent regret bound of $O\big(\frac{K\log T\Delta^{2}}{\Delta}\big)$, which is better than that of UCB1. On the other hand, the worst case regret of UCB-Improved is $O\big(\sqrt{TK\log K}\big)$. Recently in \cite{lattimore2015optimally}, the algorithm OCUCB achieves order-optimal gap-dependent regret bound of $O\left(\sum_{i=2}^{K}\frac{\log\left(T/H_i\right)}{\Delta_i}\right)$ where $H_i=\sum_{j=1}^{K}\min\lbrace \frac{1}{\Delta_i^2},\frac{1}{\Delta_j^2}\rbrace$ and gap-independent regret bound of $O\big( \sqrt{KT}\big)$.
	
\subsection*{Our Work}
We propose a variant of UCB algorithm, henceforth referred to as ClusUCB, that incorporates clustering and an improved exploration scheme. ClusUCB is a round-based algorithm that
starts with a partition of arms into small clusters, each having same number of arms. 
The clustering is done at the start with a prespecified number of clusters. 
Each round of ClusUCB involves both (individual) arm elimination as well as cluster elimination. 

%While the conditions governing the arm and cluster eliminations are inspired by UCB-Improved, the exploration factors governing these conditions are relatively more aggressive than that in UCB-Improved. 

The clustering of arms provides two benefits. First, it creates a context where UCB-Improved like algorithm can be run in parallel on smaller sets of arms with limited exploration, which could lead to fewer pulls of sub-optimal arms with the help of  more aggressive elimination of sub optimal arms. Second, the cluster elimination leads to whole sets of sub-optimal arms being simultaneously eliminated when they are found to yield poor results. These two simultaneous criteria for arm elimination can be seen as borrowing the strengths of UCB-Improved as well as other popular round based approaches.

%The motivation for our work stems from the remark in Section 2.4.3 of \cite{bubeck2012bandits}, where the authors conjecture that one should be able to obtain a bandit algorithm with a
%gap-dependent regret bound that is better than MOSS \cite{audibert2009minimax} and UCB-Improved \cite{auer2010ucb}, in particular, with a regret bound of the order 
%$O\left(\frac{K\log (\frac{T}{H})}{\Delta}\right)$, where $H = \sum_{i:\Delta_i>0} \frac{1}{\Delta_i^2}$. 

While ClusUCB does not achieve the gap-dependent regret bound of OCUCB, the theoretical analysis establishes that the gap-dependent regret of ClusUCB is always better than that of UCB-Improved and better than that of MOSS when $\sqrt{\frac{K}{7T}} \leq \Delta\leq 1$ (see Table \ref{tab:regret-bds}). Moreover, the gap-independent bound of ClusUCB is of the same order as UCB-Improved, i.e., $O\left(\sqrt{KT\log K}\right)$. However, ClusUCB is not able to match the gap-independent bound of $O(\sqrt{KT})$ for MOSS and OCUCB. We also establish the exact values for the exploration parameters and the number of clusters required for optimal behavior in the corollaries.


% 	To counter early exploration in \cite{liu2016modification} as well as in our algorithm we propose an exploration regulatory factor to control exploration. \textit{Our algorithm is also not an anytime algorithm}(neither is MOSS, UCB-Improved) and in this context we point out that knowledge of the horizon actually facilitates learning as it can exploit more information(as stated in \cite{lattimore2015optimally}). We also employ a couple of more strategies to bring down our regret as summarized below:-


\begin{table}
\caption{Gap-dependent regret bounds for different bandit algorithms}
\label{tab:regret-bds}
\begin{center}
\begin{tabular}{|c|c|}
\toprule
Algorithm  & Upper bound \\
\midrule
UCB1         &$O\left(\frac{K\log T}{\Delta}\right)$ \\\midrule
UCB-Improved &$O\left(\frac{K\log (T\Delta^{2})}{\Delta}\right)$ \\\midrule
MOSS	     &$O\left(\frac{K^{2}\log\left(T\Delta^{2}/K\right)}{\Delta}\right)$\\\midrule
ClusUCB$\big /$EClusUCB      &$O\left(\frac{K\log\left(\frac{T\Delta^{2}}{\sqrt{\log (K)}}\right)}{\Delta}\right)$\\\bottomrule
\end{tabular}
\end{center}
\end{table}

While ClusUCB is a round-based algorithm, we also introduce Efficient ClusUCB or EClusUCB which has the same theoretical guarantees as ClusUCB but empirically behaves much better. Following \cite{liu2016modification}, we claim that EClusUCB is an anytime algorithm and on five synthetic setups with small gaps, we observe empirically that EClusUCB outperforms UCB-Improved\cite{auer2010ucb}, MOSS\cite{audibert2009minimax} and OCUCB\cite{lattimore2015optimally} as well as other popular stochastic bandit algorithms such as DMED\cite{honda2010asymptotically}, UCB-V\cite{audibert2009exploration}, Median Elimination\cite{even2006action}, Thompson Sampling\cite{agrawal2011analysis} and KL-UCB\cite{garivier2011kl}.

% 	Summarizing our contributions below:-
% \begin{enumerate}
% \item We propose a cluster based round-wise algorithm with two arm elimination conditions in each round.
% \item We achieved a lower regret upper bound(Theorem1,table in Appendix F) than UCB-Improved(\cite{auer2010ucb}), UCB1(\cite{auer2002finite}) and  MOSS (\cite{audibert2009minimax} which we verify both theoretically and empirically.
% \item Our algorithm also empirically compares well with DMED, UCB-Varinace and KL-UCB.
% \item In the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$'s are small and $K$ is large which is encountered frequently in web-advertising domain (as stated in \cite{garivier2011kl}) this approach has a significant advantage over other methods.
% \item The only parameter that needs to be pre-specified to the algorithm is the number of clusters to be formed but unlike KL-UCB our algorithm parameter $p$ is not distribution-specific and also our algorithm does not involve calculation of a complex, time consuming function like the divergence function of KL-UCB.
% %though the authors specified in \cite{garivier2011kl} that for optimal result only one should use the divergence function specific to the type of distribution.
% %\item We also provide a short discussion on what other applications our algorithm can be employed successfully.
% \end{enumerate}
	
The rest of the paper is organized as follows: In Section \ref{sec:clusucb}, we present the ClusUCB algorithm and in  Section \ref{sec:eclusucb} we introduce EClusUCB. In Section \ref{sec:results}, we present the associated regret bounds and prove the main theorem on the regret upper bound for ClusUCB in Section \ref{sec:proofTheorem}. In Section \ref{sec:expts}, we present the numerical experiments and provide concluding remarks in Section \ref{sec:conclusions}. Further proofs of corollaries, theorems and proposition presented in Section \ref{sec:proofTheorem} are provided in the appendices. The algorithm Adaptive ClusUCB is presented in Appendix \ref{App:AClusUCB} and more experiments are presented in Appendix \ref{App:MoreExp}. 

%Appendix \ref{App:A}, \ref{App:B} deals with proofs of  2  propositions which are derived from our main regret bound theorem. Appendix \ref{App:Proof:Corollary:1}, \ref{App:Proof:Corollary:2}, \ref{App:Proof:Corollary:3} deals with proofs of 3 corollaries which specializes the result of our main regret bound theorem. Appendix \ref{App:D} deals with exploration regulatory factor and appendix \ref{App:E} explains why we do clustering. The last appendix \ref{App:Further:Expt} deals with further experiments on two different testbeds.
