In this paper, we consider the stochastic multi-armed bandit problem, a classical problem in sequential decision making. In this setting,  a learning algorithm is provided with a set of decisions (or arms) with reward distributions unknown to the algorithm. The learning proceeds in an iterative fashion, where in each round, the algorithm chooses an arm and receives a stochastic reward that is drawn from a stationary distribution specific to the arm selected.  
Given the goal of maximizing the cumulative reward, the learning algorithm faces the exploration-exploitation dilemma, i.e., in each round should the algorithm select the arm which has the highest observed mean reward so far 
(\textit{exploitation}), or should the algorithm choose a new arm to gain more knowledge of the true mean reward of the arms and thereby avert a sub-optimal greedy decision (\textit{exploration}). 

Formally, let $r_i$, $i=1,\ldots,K$ denote the mean rewards of the $K$ arms and $r^* = \max_i r_i$ the optimal mean reward. The objective in the stochastic bandit problem is to minimize the cumulative regret, which is defined as follows:
\begin{align*}
R_{T}=r^{*}T - \sum_{i\in A} r_{i}N_{i}(T),
\end{align*}
where $T$ is the number of rounds, $N_{i}(T)=\sum_{m=1}^n I(I_m=i)$ is the number of times the algorithm chose arm $i$ up to round $T$.
The expected regret of an algorithm after $T$ rounds can be written as
%\newline
%\newline
\begin{align*}
\E[R_{T}]= \sum_{i=1}^K \E[N_i(T)] \Delta_i,
\end{align*}
where $\Delta_{i}=r^{*}-r_{i}$ denotes the gap between the means of the optimal arm and of the $i$-th arm. 


%The problem, gets more difficult when the $\Delta_{i}$'s are smaller and arm set is larger. Also let $\Delta=min_{i\in A}\Delta_{i}$, that is it is the minimum possible gap over all arms in $A$.
                                                                                                                                          

%NEW RELATED WORK AND CONTRIBUTION

An early work involving a bandit setup is \cite{thompson1933likelihood}, where the author deals the problem of choosing between two treatments to administer on patients who come in sequentially. Following the seminal work of Robbins \cite{robbins1952some}, bandit algorithms have been extensively studied in a variety of applications. 
From a theoretical standpoint, an asymptotic lower bound for the regret was established in \cite{lai1985asymptotically}. In particular, it was shown there that for any consistent allocation strategy, we have
$\liminf_{T \to \infty}\frac{\E[R_{T}]}{\log T}\geq\sum_{\{i:r_{i}<r^{*}\}}\dfrac{(r^{*}-r_{i})}{D(p_{i}||p^{*})},$
where $D(p_{i}||p^{*})$ is the Kullback-Leibler divergence between the reward densities $p_{i}$ and $p^{*}$, corresponding to arms with mean $r_{i}$ and $r^{*}$, respectively.

	There have been several algorithms with strong regret guarantees. The foremost among them is UCB1 \cite{auer2002finite}, which has a regret upper bound of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$, where $\Delta = \min_{i:\Delta_i>0} \Delta_i$. This result is asymptotically order-optimal for the class of distributions considered. However, the worst case gap independent regret bound of UCB1  can be as bad as $O \bigg(\sqrt{TK\log T}\bigg)$.  In \cite{audibert2009minimax}, the authors propose the MOSS algorithm and establish that the worst case regret of MOSS is $O\bigg(\sqrt{TK}\bigg)$ which improves upon UCB1 by a factor of order $\sqrt{\log T}$. However, the gap-dependent regret of MOSS is  $O\left(\dfrac{K^{2}\log\left(T\Delta^{2}/K\right)}{\Delta}\right)$ and in certain regimes, this can be worse than even UCB1 (see \cite{audibert2009minimax},\cite{lattimore2015optimally}). The UCB-Improved algorithm, proposed in \cite{auer2010ucb}, is a round-based algorithm\footnote{An algorithm is \textit{round-based} if it pulls all the arms equal number of times in each round and then proceeds to eliminate one or more arms that it identifies to be sub-optimal.} variant of UCB1 that 
has a gap-dependent regret bound of $O\bigg(\dfrac{K\log T\Delta^{2}}{\Delta}\bigg)$, which is better than that of UCB1. On the other hand, the worst case regret of UCB-Improved is $O\bigg(\sqrt{TK\log K}\bigg)$. 
%\todos{What regime is this? Can you give a reference here?} 
	
	%But this algorithm performs worse empirically as stated in \cite{lattimore2015optimally}. \textit{In this work we try to address one of the open questions as raised in \cite{bubeck2012regret} that is to find an algorithm with regret always better than MOSS and UCB-Improved.}
	%
	%In this context of Multi-Armed Stochastic Bandit, we will also like to mention some of the variance based algorithm such as UCB-Normal(\cite{auer2002finite}) , UCB-Tuned(\cite{auer2002finite}) and UCB-Variance(\cite{audibert2009exploration}) which shows that variance-aware algorithms tend to perform better than the algorithms that don't(UCB1, UCB-Improved, MOSS). It can be shown that  when the variance of some sub-optimal arm is lower, a variance-aware algorithm detects it quickly thereby reducing  regret. \textit{We test our algorithm against such variance-aware techniques and do an empirical analysis.} 
	%
	%A discussion on some recent algorithms employing divergence based techniques must also be done. Firstly, the algorithm proposed in \cite{honda2010asymptotically} the authors come up with the algorithm Deterministic Minimum Empirical Divergence also called DMED$+$(as referred by \cite{garivier2011kl}) which is first order optimal(they only give asymptotic guarantees). This algorithm keeps a track of arms whose empirical mean are close to the optimal arm and takes help of large deviation ideas to find the optimal arm. Secondly, the more recent algorithm called KL-UCB(\cite{garivier2011kl}) using KL-Divergence comes up with an upper bound on regret as $\sum_{i:\Delta_{i}>0}\bigg(\dfrac{\Delta_{i}(1+\alpha)\log T}{D(r_{i},r^{*})}+C_{1}\log\log T+\dfrac{C_{2}(\alpha)}{T^{\beta(\alpha)}}\bigg)$ which is strictly better than UCB1 as we know from Pinsker's inequality $D(r_{i},r^{*}) > 2\Delta_{i}^{2}$. KL-UCB beats UCB1, MOSS and UCB-Tuned in various scenarios. \textit{We empirically test against this algorithm 
% and show that in certain regimes our algorithm performs better than KL-UCB and DMED.}
	%
	%We also make a distinction between round-based algorithms like UCB-Improved, Successive Reject(\cite{audibert2010best}), Successive Elimination(\cite{even2006action}) and Median-Elimination(\cite{even2006action}) whereby the algorithm pulls all the arms equal number of times in each round/phase, then proceeds to eliminate some number of arms it identifies to be sub-optimal with high probability and this continues till you are left with one arm. \textit{Our algorithm is also a round based algorithm}. Finally, we must point out that our setup is strictly limited to stochastic scenario as opposed to adversarial setup as studied in the paper \cite{auer2002nonstochastic} where an adversary sets the reward for the arms for every timestep.
	
\subsection*{Our Work}
We propose a variant of UCB algorithm, henceforth referred to as ClusUCB, that incorporates clustering and an improved exploration scheme. ClusUCB is a round-based algorithm that
starts with a partition of the arms into small clusters, each having same number of arms. 
The clustering is done at the start with a pre-specified number of clusters. 
Each round of ClusUCB involves both (individual) arm elimination as well as cluster elimination. 

%While the conditions governing the arm and cluster eliminations are inspired by UCB-Improved, the exploration factors governing these conditions are relatively more aggressive than that in UCB-Improved. 

The clustering of arms provides two benefits. First, it creates a context where UCB-Improved like algorithm can be run in parallel on smaller sets of arms with limited exploration, which could lead to fewer pulls of sub-optimal arms with the help of  more aggressive elimination of sub optimal arms. Second, the cluster elimination leads to whole sets of sub-optimal arms being simultaneously eliminated when they are found to yield poor results. These two simultaneous criteria for arm elimination can be seen as borrowing the strengths of UCB-Improved as well as other popular round based approaches.

The motivation for our work stems from the remark in Section 2.4.3 of \cite{bubeck2012bandits}, where the authors conjecture that one should be able to obtain a bandit algorithm with a
gap-dependent regret bound that is better than MOSS \cite{audibert2009minimax} and UCB-Improved \cite{auer2010ucb}, in particular, with a regret bound of the order 
$O\left(\dfrac{K\log (\frac{T}{H})}{\Delta}\right)$, where $H = \sum_{i:\Delta_i>0} \frac{1}{\Delta_i^2}$. While ClusUCB does not achieve the conjectured regret bound, 
the theoretical analysis establishes
% that involves carefully setting the exploration factors for arm and cluster elimination conditions, we observe
that the gap-dependent regret of ClusUCB is always better than that of UCB-Improved and better than that of MOSS when $\sqrt{\frac{e}{T}} \leq \Delta\leq 1$ (see Table \ref{tab:regret-bds}). Moreover, the gap-independent bound of ClusUCB is of the same order as UCB-Improved, i.e., $O\left(\sqrt{KT\log K}\right)$. However, ClusUCB is not able to match the gap-independent bound of $O(\sqrt{KT})$ for MOSS.


% 	To counter early exploration in \cite{liu2016modification} as well as in our algorithm we propose an exploration regulatory factor to control exploration. \textit{Our algorithm is also not an anytime algorithm}(neither is MOSS, UCB-Improved) and in this context we point out that knowledge of the horizon actually facilitates learning as it can exploit more information(as stated in \cite{lattimore2015optimally}). We also employ a couple of more strategies to bring down our regret as summarized below:-


\begin{table}
\caption{Gap-dependent regret bounds for different bandit algorithms}
\label{tab:regret-bds}
\begin{center}
\begin{tabular}{|c|c|}
\toprule
Algorithm  & Upper bound \\
\midrule
UCB1         &$O\left(\dfrac{K\log T}{\Delta}\right)$ \\\midrule
UCB-Improved &$O\left(\dfrac{K\log (T\Delta^{2})}{\Delta}\right)$ \\\midrule
MOSS	     &$O\left(\dfrac{K^{2}\log\left(T\Delta^{2}/K\right)}{\Delta}\right)$\\\midrule
ClusUCB      &$O\left(\dfrac{K\log\left(\frac{T\Delta^{2}}{\sqrt{\log (K)}}\right)}{\Delta}\right)$\\\bottomrule
\end{tabular}
\end{center}
\end{table}


On four synthetic setups with small gaps, we observe empirically that ClusUCB outperforms UCB-Improved\cite{auer2010ucb} and MOSS\cite{audibert2009minimax} as well as other popular stochastic bandit algorithms such as DMED\cite{honda2010asymptotically}, UCB-V\cite{audibert2009exploration}, Median Elimination\cite{even2006action}, Thompson Sampling\cite{agrawal2011analysis} and KL-UCB\cite{garivier2011kl}.

% 	Summarizing our contributions below:-
% \begin{enumerate}
% \item We propose a cluster based round-wise algorithm with two arm elimination conditions in each round.
% \item We achieved a lower regret upper bound(Theorem1,table in Appendix F) than UCB-Improved(\cite{auer2010ucb}), UCB1(\cite{auer2002finite}) and  MOSS (\cite{audibert2009minimax} which we verify both theoretically and empirically.
% \item Our algorithm also empirically compares well with DMED, UCB-Varinace and KL-UCB.
% \item In the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$'s are small and $K$ is large which is encountered frequently in web-advertising domain (as stated in \cite{garivier2011kl}) this approach has a significant advantage over other methods.
% \item The only parameter that needs to be pre-specified to the algorithm is the number of clusters to be formed but unlike KL-UCB our algorithm parameter $p$ is not distribution-specific and also our algorithm does not involve calculation of a complex, time consuming function like the divergence function of KL-UCB.
% %though the authors specified in \cite{garivier2011kl} that for optimal result only one should use the divergence function specific to the type of distribution.
% %\item We also provide a short discussion on what other applications our algorithm can be employed successfully.
% \end{enumerate}
	
The rest of the paper is organized as follows: In Section \ref{sec:clusucb}, we present the ClusUCB algorithm. In Section \ref{sec:results}, we present the associated regret bounds and prove the main theorem on the regret upper bound  for ClusUCB in Section \ref{sec:proofTheorem}. In Section \ref{sec:expts}, we present the numerical experiments and provide concluding remarks in Section \ref{sec:conclusions}. Further proofs of corollaries and propositions presented in Section \ref{sec:proofTheorem} are provided in the appendices. 

%Appendix \ref{App:A}, \ref{App:B} deals with proofs of  2  propositions which are derived from our main regret bound theorem. Appendix \ref{App:Proof:Corollary:1}, \ref{App:Proof:Corollary:2}, \ref{App:Proof:Corollary:3} deals with proofs of 3 corollaries which specializes the result of our main regret bound theorem. Appendix \ref{App:D} deals with exploration regulatory factor and appendix \ref{App:E} explains why we do clustering. The last appendix \ref{App:Further:Expt} deals with further experiments on two different testbeds.
